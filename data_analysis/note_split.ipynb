{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_dataframe(df):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        study = row['study']\n",
    "        note = str(row['impression']) + '\\n\\n' + str(row['findings'])\n",
    "        document = Document(page_content=note, metadata={\"study\": study})\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_by_sentences(documents):\n",
    "    split_documents = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Use nltk to split the document content into sentences\n",
    "        sentences = nltk.sent_tokenize(doc.page_content)\n",
    "        \n",
    "        split_documents.append(sentences)\n",
    "    \n",
    "    return split_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences(split_documents):\n",
    "    num_sentences = [len(doc) for doc in split_documents]\n",
    "    \n",
    "    avg_sentences = np.mean(num_sentences)\n",
    "    std_sentences = np.std(num_sentences)\n",
    "    max_sentences = np.max(num_sentences)\n",
    "    min_sentences = np.min(num_sentences)\n",
    "    \n",
    "    return avg_sentences, std_sentences, max_sentences, min_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences_evenly(sentences, num_chunks):\n",
    "    # If the number of sentences is less than the number of chunks, combine all in the first chunk\n",
    "    if len(sentences) < num_chunks:\n",
    "        return [' '.join(sentences)] + [''] * (num_chunks - 1)\n",
    "    else:\n",
    "        # Calculate the chunk sizes\n",
    "        avg_chunk_size = len(sentences) // num_chunks\n",
    "        remainder = len(sentences) % num_chunks\n",
    "        \n",
    "        chunk_sizes = [avg_chunk_size + 1 if i < remainder else avg_chunk_size for i in range(num_chunks)]\n",
    "        \n",
    "        # Split the sentences into chunks\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        for size in chunk_sizes:\n",
    "            end = start + size\n",
    "            chunks.append(' '.join(sentences[start:end]))\n",
    "            start = end\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "def chunk_df(df):\n",
    "    # Combine the \"findings\" and \"impression\" columns\n",
    "    df['combined'] = df['findings'].fillna('') + '\\n\\n' + df['impression'].fillna('')\n",
    "    df['combined'] = df['combined'].str.strip()\n",
    "    df = df[df['combined'] != '']\n",
    "\n",
    "    # Initialize lists to store sentence chunks\n",
    "    chunks_list = []\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        sentences = sent_tokenize(str(row['combined']))\n",
    "        # Distribute sentences across chunks\n",
    "        chunks = split_sentences_evenly(sentences, 6)\n",
    "        chunks_list.append(chunks)\n",
    "    \n",
    "    # Create a new DataFrame with the study column, combined column, and new sentence chunk columns\n",
    "    new_df = pd.DataFrame(df[['study', 'combined']])\n",
    "    \n",
    "    # Add new columns for each sentence chunk\n",
    "    for i in range(6):\n",
    "        new_df[f'chunk_{i+1}'] = [chunks[i] if i < len(chunks) else '' for chunks in chunks_list]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the csv with the findings and impressions to load into a pandas dataframe to convert to documents and split. To process the note, we combine the findings and impressions columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectioned_notes_path = \"/opt/gpudata/mimic-cxr/mimic_cxr_sectioned.csv\"\n",
    "sectioned_notes_df = pd.read_csv(sectioned_notes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sectioned_notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectioned_notes_docs = create_documents_from_dataframe(sectioned_notes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the average number of sentences per note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sectioned_notes = split_documents_by_sentences(sectioned_notes_docs)\n",
    "sectioned_avg_sentences, sectioned_std_sentences, sectioned_max_sentences, sectioned_min_sentences = analyze_sentences(split_sectioned_notes)\n",
    "print(\"sectioned average: \", sectioned_avg_sentences, \"\\nsectioned stdv: \", sectioned_std_sentences, \"\\nsectioned max: \", sectioned_max_sentences, \"\\nsectioned min: \", sectioned_min_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sentence_chunk_notes_df = chunk_df(sectioned_notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sentence_chunk_notes_path = \"/home/imadejski/ctds-search-model/data/mimic/mimic_cxr_sentence_chunk_sectioned.csv\"\n",
    "split_sentence_chunk_notes_df.to_csv(split_sentence_chunk_notes_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_sentence_chunk_notes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks_with_overlap(text, num_chunks=6, overlap=5):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    if total_sentences <= num_chunks:\n",
    "        # If there are fewer sentences than chunks, split sentences into smaller parts\n",
    "        words = text.split()\n",
    "        total_words = len(words)\n",
    "        chunk_size = total_words // num_chunks\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = max(0, i * chunk_size - i * overlap)\n",
    "            end_idx = min(start_idx + chunk_size + overlap, total_words)\n",
    "            chunk = words[start_idx:end_idx]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "    else:\n",
    "        # Normal chunking based on sentences with word overlap\n",
    "        chunk_size = total_sentences // num_chunks\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size\n",
    "            if i < num_chunks - 1:\n",
    "                end_idx = min(end_idx + 1, total_sentences)  # Ensure overlap of one sentence\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "            chunk_text = \" \".join(chunk_sentences)\n",
    "            \n",
    "            if i > 0:\n",
    "                # Add overlap of 5 words from the previous chunk\n",
    "                prev_chunk_words = chunks[-1].split()\n",
    "                overlap_words = \" \".join(prev_chunk_words[-overlap:])\n",
    "                chunk_text = overlap_words + \" \" + chunk_text\n",
    "            \n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def transform_dataframe(df, output_csv_path):\n",
    "    # Create an empty list to collect rows\n",
    "    rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        study = row['study']\n",
    "        full_note = f\"{row['impression']}\\n\\n{row['findings']}\"\n",
    "        chunks = split_into_chunks(full_note, num_chunks=6, overlap=5)\n",
    "        \n",
    "        # Create a dictionary for the row with dynamic chunk columns\n",
    "        row_dict = {'study': study, 'full_note': full_note}\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            row_dict[f'chunk{i+1}'] = chunk\n",
    "        \n",
    "        rows.append(row_dict)\n",
    "    \n",
    "    # Convert the list of rows into a dataframe\n",
    "    transformed_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save the dataframe to a CSV file\n",
    "    transformed_df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return transformed_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "himl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
