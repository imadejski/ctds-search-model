{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can run a paired t-test on the BioViL vs BioViL-T models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biovil_false_igl_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_0_False_igl_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovil_false_igl_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_1_False_igl_tg/model_top_n_accuracy_results.csv\")\n",
    "biovil_false_ig_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_2_False_ig_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovil_false_ig_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_3_False_ig_tg/model_top_n_accuracy_results.csv\")\n",
    "biovil_true_igl_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_0_True_igl_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovil_true_igl_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_1_True_igl_tg/model_top_n_accuracy_results.csv\")\n",
    "biovil_true_ig_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_2_True_ig_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovil_true_ig_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v2/model_run_3_True_ig_tg/model_top_n_accuracy_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biovilt_false_igl_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_4_False_igl_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovilt_false_igl_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_5_False_igl_tg/model_top_n_accuracy_results.csv\")\n",
    "biovilt_false_ig_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_6_False_ig_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovilt_false_ig_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_7_False_ig_tg/model_top_n_accuracy_results.csv\")\n",
    "biovilt_true_igl_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_0_True_igl_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovilt_true_igl_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_1_True_igl_tg/model_top_n_accuracy_results.csv\")\n",
    "biovilt_true_ig_tgl_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_2_True_ig_tgl/model_top_n_accuracy_results.csv\")\n",
    "biovilt_true_ig_tg_df = pd.read_csv(\"/opt/gpudata/imadejski/search-model/ctds-search-model/data/param_search_v3_biovilt/model_run_3_True_ig_tg/model_top_n_accuracy_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biovil_list = [biovil_false_igl_tgl_df, biovil_false_igl_tg_df, biovil_false_ig_tgl_df, biovil_false_ig_tg_df, biovil_true_igl_tgl_df, biovil_true_igl_tg_df, biovil_true_ig_tgl_df, biovil_true_ig_tg_df]\n",
    "biovilt_list = [biovilt_false_igl_tgl_df, biovilt_false_igl_tg_df, biovilt_false_ig_tgl_df, biovilt_false_ig_tg_df, biovilt_true_igl_tgl_df, biovilt_true_igl_tg_df, biovilt_true_ig_tgl_df, biovilt_true_ig_tg_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "        \"Atelectasis\",\n",
    "        \"Cardiomegaly\",\n",
    "        \"Consolidation\",\n",
    "        \"Edema\",\n",
    "        \"Enlarged Cardiomediastinum\",\n",
    "        \"Fracture\",\n",
    "        \"Lung Lesion\",\n",
    "        \"Lung Opacity\",\n",
    "        \"No Finding\",\n",
    "        \"Pleural Effusion\",\n",
    "        \"Pleural Other\",\n",
    "        \"Pneumonia\",\n",
    "        \"Pneumothorax\",\n",
    "        \"Support Devices\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biovil_biovilt_t_test(biovil_list, biovilt_list):\n",
    "    all_biovil_scores = []\n",
    "    all_biovilt_scores = []\n",
    "\n",
    "    for i in range(len(biovil_list)):\n",
    "        biovil_df = biovil_list[i].iloc[:, 1:]\n",
    "        biovilt_df = biovilt_list[i].iloc[:, 1:]\n",
    "\n",
    "        biovil_scores = biovil_df.to_numpy().flatten()\n",
    "        biovilt_scores = biovilt_df.to_numpy().flatten()\n",
    "\n",
    "        all_biovil_scores.extend(biovil_scores)\n",
    "        all_biovilt_scores.extend(biovilt_scores)\n",
    "\n",
    "    all_biovil_scores = np.array(all_biovil_scores)\n",
    "    all_biovilt_scores = np.array(all_biovilt_scores)\n",
    "\n",
    "    t_statistic, p_value = stats.ttest_rel(all_biovil_scores, all_biovilt_scores)\n",
    "    corrected_p_value = p_value*len(all_biovil_scores)\n",
    "    return (t_statistic, p_value, corrected_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biovil_biovilt_t_test_label(biovil_list, biovilt_list):\n",
    "    results = []\n",
    "    for label in labels:\n",
    "        combined_biovil_scores = []\n",
    "        combined_biovilt_scores = []\n",
    "\n",
    "        for i in range(len(biovil_list)):\n",
    "            biovil_df = biovil_list[i]\n",
    "            biovilt_df = biovilt_list[i]\n",
    "            \n",
    "            biovil_scores = biovil_df[biovil_df.iloc[:, 0] == label].iloc[:, 1:].to_numpy().flatten()\n",
    "            biovilt_scores = biovilt_df[biovilt_df.iloc[:, 0] == label].iloc[:, 1:].to_numpy().flatten()\n",
    "\n",
    "            combined_biovil_scores.extend(biovil_scores)\n",
    "            combined_biovilt_scores.extend(biovilt_scores)\n",
    "\n",
    "        combined_biovil_scores = np.array(combined_biovil_scores)\n",
    "        combined_biovilt_scores = np.array(combined_biovilt_scores)\n",
    "\n",
    "        t_statistic, p_value = stats.ttest_rel(combined_biovil_scores, combined_biovilt_scores)\n",
    "        corrected_p_value = p_value*len(combined_biovil_scores)\n",
    "        stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "        results.append({\n",
    "            'Label': label,\n",
    "            't-statistic': t_statistic,\n",
    "            'p-value': p_value,\n",
    "            'corrected p-value': corrected_p_value,\n",
    "            'statistically significant': stat_significant\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to account for the number of comparisons that are being made in the paired t-test, which we can do using the Bonferroni correction. This multiplies the p-value by the number of comparisons, here that is: number of models * number of pooling methods = 8*4 = 32 comparisons. We do this for each label because of the variation in accuracy data between the labels. We can also do this over all labels, where the number of comparisons is then 448."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value, corrected_p_value = biovil_biovilt_t_test(biovil_list, biovilt_list)\n",
    "print(f\"T-Statistic:{t_statistic}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "print(f\"Corrected P-Value {corrected_p_value}\")\n",
    "\n",
    "results = biovil_biovilt_t_test_label(biovil_list, biovilt_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can eliminate the pooling parameters. The first parameter is whether to take the average cosine similarity value or the max cosine similarity value of the values that exist for each study. The second parameter is whether to take the average cosine similarity value or the max cosine similarity value between the various queries that are being used to retrieve a positive image. We can run a paired t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_pooling_t_test(biovil_list, biovilt_list):\n",
    "    combined_list = biovil_list + biovilt_list\n",
    "    study_max_scores = []\n",
    "    study_avg_scores = []\n",
    "    \n",
    "    for df in combined_list:\n",
    "        study_max_df = df[['max_cosine_similarity_max_accuracy', 'average_cosine_similarity_max_accuracy']]\n",
    "        study_avg_df = df[['max_cosine_similarity_mean_accuracy', 'average_cosine_similarity_mean_accuracy']]\n",
    "\n",
    "        study_max_scores.extend(study_max_df.to_numpy().flatten())\n",
    "        study_avg_scores.extend(study_avg_df.to_numpy().flatten())\n",
    "\n",
    "    study_max_scores = np.array(study_max_scores)\n",
    "    study_avg_scores = np.array(study_avg_scores)\n",
    "\n",
    "    t_statistic, p_value = stats.ttest_rel(study_max_scores, study_avg_scores)\n",
    "    corrected_p_value = p_value*len(study_max_scores)\n",
    "    return (t_statistic, p_value, corrected_p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_pooling_t_test_label(biovil_list, biovilt_list):\n",
    "    results = []\n",
    "    combined_list = biovil_list + biovilt_list\n",
    "\n",
    "    for label in labels:\n",
    "        study_max_scores = []\n",
    "        study_avg_scores = []\n",
    "    \n",
    "        for df in combined_list:\n",
    "            label_df = df[df['Label'] == label]\n",
    "\n",
    "            study_max_df = label_df[['max_cosine_similarity_max_accuracy', 'average_cosine_similarity_max_accuracy']]\n",
    "            study_avg_df = label_df[['max_cosine_similarity_mean_accuracy', 'average_cosine_similarity_mean_accuracy']]\n",
    "\n",
    "            study_max_scores.extend(study_max_df.to_numpy().flatten())\n",
    "            study_avg_scores.extend(study_avg_df.to_numpy().flatten())\n",
    "\n",
    "        study_max_scores = np.array(study_max_scores)\n",
    "        study_avg_scores = np.array(study_avg_scores)\n",
    "\n",
    "        t_statistic, p_value = stats.ttest_rel(study_max_scores, study_avg_scores)\n",
    "        corrected_p_value = p_value*len(study_max_scores)\n",
    "        stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "        results.append({\n",
    "            'Label': label,\n",
    "            't-statistic': t_statistic,\n",
    "            'p-value': p_value,\n",
    "            'corrected_p-value': corrected_p_value,\n",
    "            'statistically significant': stat_significant\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value, corrected_p_value = study_pooling_t_test(biovil_list, biovilt_list)\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Corrected p-value: {corrected_p_value}\")\n",
    "\n",
    "results = study_pooling_t_test_label(biovil_list, biovilt_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pooling_t_test(biovil_list, biovilt_list):\n",
    "    combined_list = biovil_list + biovilt_list\n",
    "    query_max_scores = []\n",
    "    query_avg_scores = []\n",
    "    \n",
    "    for df in combined_list:\n",
    "        query_max_df = df[['max_cosine_similarity_max_accuracy', 'max_cosine_similarity_mean_accuracy']]\n",
    "        query_avg_df = df[['average_cosine_similarity_max_accuracy', 'average_cosine_similarity_mean_accuracy']]\n",
    "\n",
    "        query_max_scores.extend(query_max_df.to_numpy().flatten())\n",
    "        query_avg_scores.extend(query_avg_df.to_numpy().flatten())\n",
    "\n",
    "    query_max_scores = np.array(query_max_scores)\n",
    "    query_avg_scores = np.array(query_avg_scores)\n",
    "\n",
    "    t_statistic, p_value = stats.ttest_rel(query_max_scores, query_avg_scores)\n",
    "    corrected_p_value = p_value*len(query_max_scores)\n",
    "    return (t_statistic, p_value, corrected_p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pooling_t_test_label(biovil_list, biovilt_list):\n",
    "    results = []\n",
    "    combined_list = biovil_list + biovilt_list\n",
    "\n",
    "    for label in labels:\n",
    "        query_max_scores = []\n",
    "        query_avg_scores = []\n",
    "    \n",
    "        for df in combined_list:\n",
    "            label_df = df[df['Label'] == label]\n",
    "\n",
    "            query_max_df = label_df[['max_cosine_similarity_max_accuracy', 'max_cosine_similarity_mean_accuracy']]\n",
    "            query_avg_df = label_df[['average_cosine_similarity_max_accuracy', 'average_cosine_similarity_mean_accuracy']]\n",
    "\n",
    "            query_max_scores.extend(query_max_df.to_numpy().flatten())\n",
    "            query_avg_scores.extend(query_avg_df.to_numpy().flatten())\n",
    "\n",
    "        query_max_scores = np.array(query_max_scores)\n",
    "        query_avg_scores = np.array(query_avg_scores)\n",
    "\n",
    "        t_statistic, p_value = stats.ttest_rel(query_max_scores, query_avg_scores)\n",
    "        corrected_p_value = p_value*len(query_max_scores)\n",
    "        stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "        results.append({\n",
    "            'Label': label,\n",
    "            't-statistic': t_statistic,\n",
    "            'p-value': p_value,\n",
    "            'corrected_p-value': corrected_p_value,\n",
    "            'statistically significant': stat_significant\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value, corrected_p_value = query_pooling_t_test(biovil_list, biovilt_list)\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Corrected p-value: {corrected_p_value}\")\n",
    "\n",
    "results = query_pooling_t_test_label(biovil_list, biovilt_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can run an ANOVA test on the pooling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_method_anova(biovil_list, biovilt_list):\n",
    "    model_accuracy_dfs = biovil_list + biovilt_list\n",
    "    long_dfs = []\n",
    "    for idx, df in enumerate(model_accuracy_dfs):\n",
    "        long_df = df.melt(id_vars=['Label'], var_name='Pooling_Method', value_name='Accuracy')\n",
    "        long_df['Model'] = f'Model_{idx + 1}'  # Label each model uniquely\n",
    "        long_dfs.append(long_df)\n",
    "\n",
    "    combined_df = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "    model = ols('Accuracy ~ Pooling_Method', data=combined_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "    return anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_method_anova_label(biovil_list, biovilt_list):\n",
    "    model_accuracy_dfs = biovil_list + biovilt_list\n",
    "    long_dfs = []\n",
    "\n",
    "    for idx, df in enumerate(model_accuracy_dfs):\n",
    "        long_df = df.melt(id_vars=['Label'], var_name='Pooling_Method', value_name='Accuracy')\n",
    "        long_df['Model'] = f'Model_{idx + 1}'  # Label each model uniquely\n",
    "        long_dfs.append(long_df)\n",
    "\n",
    "    combined_df = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "    anova_results = {}\n",
    "\n",
    "    for label in labels:\n",
    "        label_data = combined_df[combined_df['Label'] == label]\n",
    "\n",
    "        model = ols('Accuracy ~ Pooling_Method', data=label_data).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "        anova_results[label] = anova_table\n",
    "\n",
    "    return anova_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = pooling_method_anova(biovil_list, biovilt_list)\n",
    "print(anova)\n",
    "\n",
    "label_anova = pooling_method_anova_label(biovil_list, biovilt_list)\n",
    "for label in labels:\n",
    "    print(f\"{label}:\\n\", label_anova[label], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_t_test_label_all(biovil_list, biovilt_list):\n",
    "    results = []\n",
    "    combined_list = biovil_list + biovilt_list\n",
    "\n",
    "    pair_list = [\n",
    "        ['max_cosine_similarity_max_accuracy', 'max_cosine_similarity_mean_accuracy'], \n",
    "        ['max_cosine_similarity_max_accuracy', 'average_cosine_similarity_max_accuracy'],\n",
    "        ['max_cosine_similarity_max_accuracy', 'average_cosine_similarity_mean_accuracy'],\n",
    "        ['average_cosine_similarity_max_accuracy', 'average_cosine_similarity_mean_accuracy'],\n",
    "        ['average_cosine_similarity_max_accuracy', 'max_cosine_similarity_mean_accuracy'],\n",
    "        ['max_cosine_similarity_mean_accuracy', 'average_cosine_similarity_mean_accuracy']\n",
    "    ]\n",
    "    for pair in pair_list:\n",
    "        for label in labels:\n",
    "            pool_1_scores = []\n",
    "            pool_2_scores = []\n",
    "        \n",
    "            for df in combined_list:\n",
    "                label_df = df[df['Label'] == label]\n",
    "\n",
    "                pool_1_df = label_df[pair[0]]\n",
    "                pool_2_df = label_df[pair[1]]\n",
    "\n",
    "                pool_1_scores.extend(pool_1_df.to_numpy().flatten())\n",
    "                pool_2_scores.extend(pool_2_df.to_numpy().flatten())\n",
    "\n",
    "            pool_1_scores = np.array(pool_1_scores)\n",
    "            pool_2_scores = np.array(pool_2_scores)\n",
    "\n",
    "            t_statistic, p_value = stats.ttest_rel(pool_1_scores, pool_2_scores)\n",
    "            corrected_p_value = p_value*len(pool_1_scores)\n",
    "            stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "            results.append({\n",
    "                'Label': label,\n",
    "                'Pooling_Method_1': pair[0],\n",
    "                'Pooling_Method_2': pair[1],\n",
    "                't-statistic': t_statistic,\n",
    "                'p-value': p_value,\n",
    "                'corrected_p-value': corrected_p_value,\n",
    "                'statistically significant': stat_significant\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pooling_t_test_label_all(biovil_list, biovilt_list)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_pooling_method(pooling_method_t_test_label_results):\n",
    "    pooling_method_wins = defaultdict(int)\n",
    "    \n",
    "    for index, row in pooling_method_t_test_label_results.iterrows():\n",
    "        if row['statistically significant']:\n",
    "            if row['t-statistic'] > 0:\n",
    "                pooling_method_wins[row['Pooling_Method_1']] += 1\n",
    "            elif row['t-statistic'] < 0:\n",
    "                pooling_method_wins[row['Pooling_Method_2']] += 1\n",
    "\n",
    "    pooling_method_wins_df = pd.DataFrame.from_dict(pooling_method_wins, orient='index', columns=['Wins'])\n",
    "    pooling_method_wins_df = pooling_method_wins_df.sort_values(by='Wins', ascending=False)\n",
    "    \n",
    "    return pooling_method_wins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = query_pooling_t_test_label_all(biovil_list, biovilt_list)\n",
    "pooling_method_win = determine_pooling_method(results_df)\n",
    "print(pooling_method_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run a paired t-test to decide whether we should average the patches after projection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_false = [biovil_false_igl_tgl_df, biovil_false_igl_tg_df, biovil_false_ig_tgl_df, biovil_false_ig_tg_df, biovilt_false_igl_tgl_df, biovilt_false_igl_tg_df, biovilt_false_ig_tgl_df, biovilt_false_ig_tg_df]\n",
    "list_true = [biovil_true_igl_tgl_df, biovil_true_igl_tg_df, biovil_true_ig_tgl_df, biovil_true_ig_tg_df, biovilt_true_igl_tgl_df, biovilt_true_igl_tg_df, biovilt_true_ig_tgl_df, biovilt_true_ig_tg_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_avg_t_test(list_false, list_true):\n",
    "    false_scores = []\n",
    "    true_scores = []\n",
    "\n",
    "    for i in range(len(list_false)):\n",
    "        false_df = list_false[i]\n",
    "        true_df = list_true[i]\n",
    "\n",
    "        false_df = false_df[['max_cosine_similarity_max_accuracy']]\n",
    "        true_df = true_df[['max_cosine_similarity_max_accuracy']]\n",
    "\n",
    "        false_scores.extend(false_df.to_numpy().flatten())\n",
    "        true_scores.extend(true_df.to_numpy().flatten())\n",
    "        \n",
    "    false_scores = np.array(false_scores)\n",
    "    true_scores = np.array(true_scores)\n",
    "\n",
    "    t_statistic, p_value = stats.ttest_rel(false_scores, true_scores)\n",
    "    corrected_p_value = p_value*len(false_scores)\n",
    "    return (t_statistic, p_value, corrected_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_avg_t_test_label(list_false, list_true):\n",
    "    results = []\n",
    "\n",
    "    for label in labels:\n",
    "        false_scores = []\n",
    "        true_scores = []\n",
    "\n",
    "        for i in range(len(list_false)):\n",
    "            false_df = list_false[i]\n",
    "            true_df = list_true[i]\n",
    "\n",
    "            false_label_df = false_df[false_df['Label'] == label]\n",
    "            true_label_df = true_df[true_df['Label'] == label]\n",
    "\n",
    "            false_scores.extend(false_label_df[['max_cosine_similarity_max_accuracy']].to_numpy().flatten())\n",
    "            true_scores.extend(true_label_df[['max_cosine_similarity_max_accuracy']].to_numpy().flatten())\n",
    "        \n",
    "        false_scores = np.array(false_scores)\n",
    "        true_scores = np.array(true_scores)\n",
    "\n",
    "        t_statistic, p_value = stats.ttest_rel(false_scores, true_scores)\n",
    "        corrected_p_value = p_value * len(false_scores)\n",
    "        stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "        results.append({\n",
    "            'Label': label,\n",
    "            't-statistic': t_statistic,\n",
    "            'p-value': p_value,\n",
    "            'corrected p-value': corrected_p_value,\n",
    "            'statistically significant': stat_significant\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value, corrected_p_value = proj_avg_t_test(list_false, list_true)\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"Corrected P-Value: {corrected_p_value}\")\n",
    "\n",
    "results_df = proj_avg_t_test_label(list_false, list_true)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can do an ANOVA test to choose the best loss calculation combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igl_tgl_list = [biovil_true_igl_tgl_df, biovil_false_igl_tgl_df, biovilt_true_igl_tgl_df, biovilt_false_igl_tgl_df]\n",
    "igl_tg_list = [biovil_true_igl_tg_df, biovil_false_igl_tg_df, biovilt_true_igl_tg_df, biovilt_false_igl_tg_df]\n",
    "ig_tgl_list = [biovil_true_ig_tgl_df, biovil_false_ig_tgl_df, biovilt_true_ig_tgl_df, biovilt_false_ig_tgl_df]\n",
    "ig_tg_list = [biovil_true_ig_tg_df, biovil_false_ig_tg_df, biovilt_true_ig_tg_df, biovilt_false_ig_tg_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_combo_anova(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list):\n",
    "    model_types = {\n",
    "        \"IGL_TGL\": igl_tgl_list, \"IGL_TG\": igl_tg_list, \"IG_TGL\": ig_tgl_list, \"IG_TG\": ig_tg_list\n",
    "    }\n",
    "    long_dfs = []\n",
    "\n",
    "    # Iterate through each model type and the models within each type\n",
    "    for model_type_name, models in model_types.items():\n",
    "        for model_idx, df in enumerate(models):\n",
    "            # Reshape to long format, keeping the 'Label' column for reference\n",
    "            long_df = df.melt(id_vars=['Label'], var_name='Pooling_Method', value_name='Accuracy')\n",
    "            long_df['Model_Type'] = model_type_name  # Use custom model type names from your lists\n",
    "            long_df['Model'] = f'{model_type_name}_Model_{model_idx + 1}'  # Label each model uniquely\n",
    "            long_dfs.append(long_df)\n",
    "\n",
    "    # Combine all long-format DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "    # Drop the 'Label' and 'Pooling_Method' columns as they are not needed for the ANOVA\n",
    "    combined_df = combined_df.drop(columns=['Label', 'Pooling_Method'])\n",
    "\n",
    "    # Perform one-way ANOVA to analyze the effect of model type on accuracy\n",
    "    model = ols('Accuracy ~ Model_Type', data=combined_df).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "    # Output the ANOVA table\n",
    "    return anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_combo_anova_label(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list):\n",
    "    model_types = {\n",
    "        \"IGL_TGL\": igl_tgl_list, \"IGL_TG\": igl_tg_list, \"IG_TGL\": ig_tgl_list, \"IG_TG\": ig_tg_list\n",
    "    }\n",
    "    long_dfs = []\n",
    "\n",
    "    # Iterate through each model type and the models within each type\n",
    "    for model_type_name, models in model_types.items():\n",
    "        for model_idx, df in enumerate(models):\n",
    "            # Reshape to long format, keeping the 'Label' column for reference\n",
    "            long_df = df.melt(id_vars=['Label'], var_name='Pooling_Method', value_name='Accuracy')\n",
    "            long_df['Model_Type'] = model_type_name  # Use custom model type names from your lists\n",
    "            long_df['Model'] = f'{model_type_name}_Model_{model_idx + 1}'  # Label each model uniquely\n",
    "            long_dfs.append(long_df)\n",
    "\n",
    "    # Combine all long-format DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "    anova_results = {}\n",
    "\n",
    "    # Loop through each label and run ANOVA\n",
    "    for label in labels:\n",
    "        # Filter the data for the current label\n",
    "        label_data = combined_df[combined_df['Label'] == label]\n",
    "\n",
    "        # Perform one-way ANOVA for the current label\n",
    "        model = ols('Accuracy ~ Model_Type', data=label_data).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "        # Store the result for the current label\n",
    "        anova_results[label] = anova_table\n",
    "\n",
    "    # Output the ANOVA table\n",
    "    return anova_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova = loss_combo_anova(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list)\n",
    "print(\"All labels: \\n\", anova, \"\\n\")\n",
    "\n",
    "label_anova = loss_combo_anova_label(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list)\n",
    "for label in labels:\n",
    "    print(label, \": \\n\", label_anova[label], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_combo_t_test_label(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list):\n",
    "    pairwise_comparisons = [\n",
    "        (\"IGL_TGL\", \"IGL_TG\"),\n",
    "        (\"IGL_TGL\", \"IG_TGL\"),\n",
    "        (\"IGL_TGL\", \"IG_TG\"),\n",
    "        (\"IGL_TG\", \"IG_TGL\"),\n",
    "        (\"IGL_TG\", \"IG_TG\"),\n",
    "        (\"IG_TGL\", \"IG_TG\")\n",
    "    ]\n",
    "    model_types = {\n",
    "        \"IGL_TGL\": igl_tgl_list, \"IGL_TG\": igl_tg_list, \"IG_TGL\": ig_tgl_list, \"IG_TG\": ig_tg_list\n",
    "    }\n",
    "    long_dfs = []\n",
    "    \n",
    "    # Reshape all data to long format and store it in long_dfs\n",
    "    for model_type_name, models in model_types.items():\n",
    "        for model_idx, df in enumerate(models):\n",
    "            # Reshape to long format, keeping the 'Label' column for reference\n",
    "            long_df = df.melt(id_vars=['Label'], var_name='Pooling_Method', value_name='Accuracy')\n",
    "            long_df['Model_Type'] = model_type_name  # Use custom model type names from your lists\n",
    "            long_df['Model'] = f'{model_type_name}_Model_{model_idx + 1}'  # Label each model uniquely\n",
    "            long_dfs.append(long_df)\n",
    "\n",
    "    # Combine all long-format DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "    # Get unique labels from the combined DataFrame\n",
    "    labels = combined_df['Label'].unique()\n",
    "\n",
    "    # Store the t-test results for each label and pairwise combination\n",
    "    t_test_results = []\n",
    "\n",
    "    # Loop through each label and run paired t-tests for each pairwise comparison\n",
    "    for label in labels:\n",
    "        for pair in pairwise_comparisons:\n",
    "            # Filter the data for the current label and each model type in the pair\n",
    "            model_1_data = combined_df[(combined_df['Label'] == label) & (combined_df['Model_Type'] == pair[0])]\n",
    "            model_2_data = combined_df[(combined_df['Label'] == label) & (combined_df['Model_Type'] == pair[1])]\n",
    "\n",
    "            # Ensure that we have data for both model types for the current label\n",
    "            if not model_1_data.empty and not model_2_data.empty:\n",
    "                # Extract the accuracy scores for both model types\n",
    "                model_1_scores = model_1_data['Accuracy'].to_numpy().flatten()\n",
    "                model_2_scores = model_2_data['Accuracy'].to_numpy().flatten()\n",
    "\n",
    "                # Perform paired t-test between the two model types for the current label\n",
    "                t_statistic, p_value = stats.ttest_rel(model_1_scores, model_2_scores)\n",
    "                corrected_p_value = p_value*len(model_1_scores)\n",
    "                stat_significant = corrected_p_value < 0.05\n",
    "\n",
    "                # Store the result for the current label and pair of loss combinations\n",
    "                t_test_results.append({\n",
    "                    'Label': label,\n",
    "                    'Model_Type_1': pair[0],\n",
    "                    'Model_Type_2': pair[1],\n",
    "                    't-statistic': t_statistic,\n",
    "                    'p-value': p_value,\n",
    "                    'corrected p-value': corrected_p_value,\n",
    "                    'statistically significant': stat_significant\n",
    "                })\n",
    "\n",
    "    # Convert the results into a DataFrame for easier viewing\n",
    "    t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "    return t_test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loss_combo_t_test_label(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_loss_combo(loss_combo_t_test_label_results):\n",
    "    loss_combo_wins = defaultdict(int)\n",
    "    \n",
    "    for index, row in loss_combo_t_test_label_results.iterrows():\n",
    "        if row['statistically significant']:\n",
    "            if row['t-statistic'] > 0:\n",
    "                loss_combo_wins[row['Model_Type_1']] += 1\n",
    "            elif row['t-statistic'] < 0:\n",
    "                loss_combo_wins[row['Model_Type_2']] += 1\n",
    "\n",
    "    loss_combo_wins_df = pd.DataFrame.from_dict(loss_combo_wins, orient='index', columns=['Wins'])\n",
    "    loss_combo_wins_df = loss_combo_wins_df.sort_values(by='Wins', ascending=False)\n",
    "    \n",
    "    return loss_combo_wins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loss_combo_t_test_label(igl_tgl_list, igl_tg_list, ig_tgl_list, ig_tg_list)\n",
    "loss_combo_wins = determine_loss_combo(results)\n",
    "print(loss_combo_wins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
