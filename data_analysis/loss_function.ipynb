{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from health_multimodal.image.model.pretrained import get_biovil_image_encoder\n",
    "from health_multimodal.image.data.transforms import (\n",
    "    create_chest_xray_transform_for_inference,\n",
    ")\n",
    "\n",
    "from health_multimodal.image import ImageInferenceEngine\n",
    "from health_multimodal.text import TextInferenceEngine\n",
    "\n",
    "from health_multimodal.text.utils import BertEncoderType, get_bert_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE = 512\n",
    "CENTER_CROP_SIZE = 512\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can instantiate both the image and text inference engines from Microsoft's hi-ml library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model = ImageInferenceEngine(\n",
    "        image_model=get_biovil_image_encoder().to(device),\n",
    "        transform=create_chest_xray_transform_for_inference(\n",
    "            resize=RESIZE, center_crop_size=CENTER_CROP_SIZE\n",
    "        ),\n",
    ")\n",
    "text_model = get_bert_inference(BertEncoderType.BIOVIL_T_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the images, we need to load and transform the image and then retrieve both the patch and global embeddings. \n",
    "\n",
    "I.e. for one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_arange(n, repeats):\n",
    "    # Create a tensor with values from 0 to n\n",
    "    arange_tensor = torch.arange(n)\n",
    "    # Repeat each element 'repeats' times\n",
    "    repeated_tensor = arange_tensor.repeat_interleave(repeats)\n",
    "    return repeated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can write the contrastive loss function for the local to local alignment. This means that we will be comparing every single image patch embedding with every single text embedding for one specific note and image pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, image_model, text_model, temperature=0.1):\n",
    "        super(LocalContrastiveLoss, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image1, image2, queries1, queries2):\n",
    "        image_path1 = Path(image1)\n",
    "        # Get patch embeddings from the image inference engine\n",
    "        patch_embeddings1, _ = self.image_model.get_projected_patch_embeddings(image_path1)  # Shape: (batch_size, n_patches_h, n_patches_w, embed_dim)\n",
    "\n",
    "        # Get text embeddings from the text inference engine\n",
    "        text_embeddings1 = self.text_model.get_embeddings_from_prompt(queries1)  # Shape: (batch_size, num_text, embed_dim)\n",
    "\n",
    "        image_path2 = Path(image2)\n",
    "        # Get patch embeddings from the image inference engine\n",
    "        patch_embeddings2, _ = self.image_model.get_projected_patch_embeddings(image_path2)  # Shape: (batch_size, n_patches_h, n_patches_w, embed_dim)\n",
    "\n",
    "        # Get text embeddings from the text inference engine\n",
    "        text_embeddings2 = self.text_model.get_embeddings_from_prompt(queries2)  # Shape: (batch_size, num_text, embed_dim)\n",
    "\n",
    "        patch_embeddings = torch.stack((patch_embeddings1, patch_embeddings2), dim=0)\n",
    "        text_embeddings = torch.stack((text_embeddings1, text_embeddings2), dim=0)\n",
    "\n",
    "        print(\"patch emb shape after adding batch: \", patch_embeddings.shape)\n",
    "        print(\"text emb shape after adding batch: \", text_embeddings.shape)\n",
    "\n",
    "        # Ensure tensors are on the same device\n",
    "        patch_embeddings = patch_embeddings.to(device)\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        patch_embeddings = F.normalize(patch_embeddings, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "        \n",
    "        # Flatten the spatial dimensions of patch_embeddings\n",
    "        batch_size, n_patches_h, n_patches_w, embed_dim = patch_embeddings.shape\n",
    "        patch_embeddings = patch_embeddings.view(batch_size, n_patches_h * n_patches_w, embed_dim)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        batch_size, num_patches, emb_dim = patch_embeddings.shape\n",
    "        batch_size, num_text, emb_dim = text_embeddings.shape\n",
    "\n",
    "        # flatten batches\n",
    "        patch_embeddings = patch_embeddings.view(-1, 128) #shape: (batch_size * num_patches, embed_dim)\n",
    "        text_embeddings = text_embeddings.view(-1, 128) #shape: (batch_size * num_text, embed_dim)\n",
    "        \n",
    "        # Calculate dot product between every text embedding and every image embedding\n",
    "        similarities_1 = torch.einsum('pd,td->pt', patch_embeddings, text_embeddings)  # shape: (batch_size * num_patches, batch_size * num_text)\n",
    "\n",
    "        # should be (batch_size * num_patches, batch_size * num_text) -- with averaging --> (batch_size * num_patches, batch_size) for similarity 1\n",
    "        # should be \n",
    "        \n",
    "        # Temperature scaling\n",
    "        similarities_1 = similarities_1 / self.temperature\n",
    "\n",
    "        print(\"similarities 1 shape: \", similarities_1.shape)\n",
    "        \n",
    "        # Get dimensions of similarities matrix\n",
    "        batch_patches, batch_texts = similarities_1.shape\n",
    "\n",
    "        # Take average of \n",
    "        #stride_1 = num_patches\n",
    "        #stride_2 = num_text\n",
    "        #step = 1\n",
    "\n",
    "        new_shape = (batch_patches, -1, num_text)\n",
    "        reshaped_similarities_1 = similarities_1.view(new_shape)\n",
    "        #reshaped_similarities_1 = similarities_1.view(batch_size, num_patches, -1)\n",
    "        print(\"reshaped sim 1: \", reshaped_similarities_1.shape)\n",
    "        averaged_similarities_1 = reshaped_similarities_1.mean(dim=-1)\n",
    "        print(\"avg sim1: \", averaged_similarities_1.shape)\n",
    "\n",
    "        #targets = torch.arange(batch_size)\n",
    "        #targets = repeated_arange(batch_size, num_sims)\n",
    "        \n",
    "        targets_1 = torch.arange(batch_size).repeat_interleave(num_patches)\n",
    "        targets_1 = targets_1.to(device)\n",
    "        print(\"targets1 shape: \", targets_1.shape)\n",
    "\n",
    "        logits_p = averaged_similarities_1 # shape: (batch_size, num_text)\n",
    "        \n",
    "        print(\"logits p shape: \", logits_p.shape)\n",
    "        \n",
    "        # Image to text cross-entropy loss\n",
    "        loss_i_to_t = F.cross_entropy(logits_p, targets_1)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = (loss_i_to_t)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = '/opt/gpudata/mimic-cxr/jpg/p10/p10011365/s53459647/f6fccc21-ded29731-2a7419a6-961566fe-710630d3.jpg'\n",
    "queries1 = ['penumonia', 'pneumonia seen', 'pneumonia is seen', 'peumonia present']\n",
    "\n",
    "image2 = '/opt/gpudata/mimic-cxr/jpg/p10/p10000980/s50985099/6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d.jpg'\n",
    "queries2 = ['penumonia', 'pneumonia not seen', 'pneumonia is not seen', 'peumonia not present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocalContrastiveLoss(img_model, text_model).to(device)\n",
    "loss = model(image1, image2, queries1, queries2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, image_model, text_model, temperature=0.1):\n",
    "        super(LocalContrastiveLoss, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image1, image2, queries1, queries2):\n",
    "        image_path1 = Path(image1)\n",
    "        # Get patch embeddings from the image inference engine\n",
    "        patch_embeddings1, _ = self.image_model.get_projected_patch_embeddings(image_path1)  # Shape: (batch_size, n_patches_h, n_patches_w, embed_dim)\n",
    "        print(\"patch emb 1 shape originally: \", patch_embeddings1.shape)\n",
    "\n",
    "        # Get text embeddings from the text inference engine\n",
    "        text_embeddings1 = self.text_model.get_embeddings_from_prompt(queries1)  # Shape: (batch_size, num_text, embed_dim)\n",
    "        print(\"text emb 1 shape originally: \", text_embeddings1.shape)\n",
    "\n",
    "        image_path2 = Path(image2)\n",
    "        # Get patch embeddings from the image inference engine\n",
    "        patch_embeddings2, _ = self.image_model.get_projected_patch_embeddings(image_path2)  # Shape: (batch_size, n_patches_h, n_patches_w, embed_dim)\n",
    "        print(\"patch emb 2 shape originally: \", patch_embeddings2.shape)\n",
    "\n",
    "        # Get text embeddings from the text inference engine\n",
    "        text_embeddings2 = self.text_model.get_embeddings_from_prompt(queries2)  # Shape: (batch_size, num_text, embed_dim)\n",
    "        print(\"text emb 2 shape originally: \", text_embeddings2.shape)\n",
    "\n",
    "        patch_embeddings = torch.stack((patch_embeddings1, patch_embeddings2), dim=0)\n",
    "        text_embeddings = torch.stack((text_embeddings1, text_embeddings2), dim=0)\n",
    "\n",
    "        print(\"patch emb shape after adding batch: \", patch_embeddings.shape)\n",
    "        print(\"text emb shape after adding batch: \", text_embeddings.shape)\n",
    "\n",
    "        # Ensure tensors are on the same device\n",
    "        patch_embeddings = patch_embeddings.to(device)\n",
    "        text_embeddings = text_embeddings.to(device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        patch_embeddings = F.normalize(patch_embeddings, dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "        \n",
    "        # Flatten the spatial dimensions of patch_embeddings\n",
    "        batch_size, n_patches_h, n_patches_w, embed_dim = patch_embeddings.shape\n",
    "        patch_embeddings = patch_embeddings.view(batch_size, n_patches_h * n_patches_w, embed_dim)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        batch_size, num_patches, emb_dim = patch_embeddings.shape\n",
    "        batch_size, num_text, emb_dim = text_embeddings.shape\n",
    "\n",
    "        # flatten batches\n",
    "        patch_embeddings = patch_embeddings.view(-1, 128) #shape: (batch_size * num_patches, embed_dim)\n",
    "        text_embeddings = text_embeddings.view(-1, 128) #shape: (batch_size * num_text, embed_dim)\n",
    "        \n",
    "        # Calculate dot product between every text embedding and every image embedding\n",
    "        similarities_1 = torch.einsum('pd,td->pt', patch_embeddings, text_embeddings)  # shape: (batch_size * num_patches, batch_size * num_text)\n",
    "        similarities_2 = torch.einsum('td,pd->tp', text_embeddings, patch_embeddings)  # shape: (batch_size * num_text, batch_size * num_patches)\n",
    "\n",
    "        # should be (batch_size * num_patches, batch_size * num_text) -- with averaging --> (batch_size * num_patches, batch_size) for similarity 1\n",
    "        # should be \n",
    "        \n",
    "        # Temperature scaling\n",
    "        similarities_1 = similarities_1 / self.temperature\n",
    "        similarities_2 = similarities_2 / self.temperature\n",
    "\n",
    "        print(\"similarities 1 shape: \", similarities_1.shape)\n",
    "        print(\"similarities 2 shape: \", similarities_2.shape)\n",
    "        \n",
    "        # Get dimensions of similarities matrix\n",
    "        batch_patches, batch_texts = similarities_1.shape\n",
    "\n",
    "        # Take average of \n",
    "        #stride_1 = num_patches\n",
    "        #stride_2 = num_text\n",
    "        #step = 1\n",
    "\n",
    "        new_shape_1 = (batch_patches, -1, num_text)\n",
    "        reshaped_similarities_1 = similarities_1.view(new_shape_1)\n",
    "        #reshaped_similarities_1 = similarities_1.view(batch_size, num_patches, -1)\n",
    "        print(\"reshaped sim 1: \", reshaped_similarities_1.shape)\n",
    "        averaged_similarities_1 = reshaped_similarities_1.mean(dim=-1)\n",
    "        print(\"avg sim1: \", averaged_similarities_1.shape)\n",
    "\n",
    "        new_shape_2 = (batch_texts, -1, num_text)\n",
    "        reshaped_similarities_2 = similarities_2.view(new_shape_2)\n",
    "        averaged_similarities_2 = reshaped_similarities_2.mean(dim=-1)\n",
    "        print(\"avg sim2: \", averaged_similarities_2.shape)\n",
    "\n",
    "        #targets = torch.arange(batch_size)\n",
    "        #targets = repeated_arange(batch_size, num_sims)\n",
    "        \n",
    "        targets_1 = torch.arange(batch_size).repeat_interleave(num_patches)\n",
    "        targets_1 = targets_1.to(device)\n",
    "        print(\"targets1 shape: \", targets_1.shape)\n",
    "\n",
    "        targets_2 = torch.arange(batch_size).repeat_interleave(num_text)\n",
    "        targets_2 = targets_2.to(device)\n",
    "        print(\"targets2 shape: \", targets_2.shape)\n",
    "\n",
    "        logits_p = averaged_similarities_1 # shape: (batch_size, num_text)\n",
    "        logits_t = averaged_similarities_2 # shape: (batch_size, num_text)\n",
    "        \n",
    "        print(\"logits p shape: \", logits_p.shape)\n",
    "        print(\"logits t shape: \", logits_t.shape)\n",
    "        \n",
    "        # Image to text cross-entropy loss\n",
    "        loss_i_to_t = F.cross_entropy(logits_p, targets_1)\n",
    "        \n",
    "        # Text to image cross-entropy loss\n",
    "        loss_t_to_i = F.cross_entropy(logits_t, targets_2)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = (loss_i_to_t + loss_t_to_i) / 2\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make function generalizeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, image_model, text_model, temperature=1.0):\n",
    "        super(LocalContrastiveLoss, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def info_nce(self, first_embeddings, second_embeddings):\n",
    "\n",
    "        batch_size, first_num, dim = first_embeddings.shape\n",
    "        batch_size, second_num, dim = second_embeddings.shape\n",
    "\n",
    "        #flatten batches\n",
    "        first_embeddings = first_embeddings.view(-1, dim)\n",
    "        second_embeddings = second_embeddings.view(-1, dim)\n",
    "\n",
    "        similarities = torch.einsum('fd,sd->fs', first_embeddings, second_embeddings)\n",
    "        similarities = similarities / self.temperature\n",
    "\n",
    "        batch_first, batch_second = similarities.shape\n",
    "\n",
    "        new_shape = (batch_first, -1, second_num)\n",
    "        reshaped_similarities = similarities.view(new_shape)\n",
    "        avg_similarities = reshaped_similarities.mean(dim=-1)\n",
    "\n",
    "        targets = torch.arange(batch_size).repeat_interleave(first_num).to(device)\n",
    "\n",
    "        logits = avg_similarities\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, image_embeddings, patch_embeddings, text_embeddings, chunk_embeddings):\n",
    "\n",
    "        # Normalize and ensure all tensors are on the same device\n",
    "        image_embeddings = F.normalize(image_embeddings.to(device), dim=-1)\n",
    "        patch_embeddings = F.normalize(patch_embeddings.to(device), dim=-1)\n",
    "        text_embeddings = F.normalize(text_embeddings.to(device), dim=-1)\n",
    "        chunk_embeddings = F.normalize(chunk_embeddings.to(device), dim=-1)\n",
    "        \n",
    "        # Flatten the spatial dimensions of patch_embeddings\n",
    "        batch_size, n_patches_h, n_patches_w, embed_dim = patch_embeddings.shape\n",
    "        patch_embeddings = patch_embeddings.view(batch_size, n_patches_h * n_patches_w, embed_dim)  # Shape: (batch_size, n_patches, embed_dim)\n",
    "\n",
    "        loss_local_i_to_local_t = self.info_nce(patch_embeddings, chunk_embeddings)\n",
    "        loss_local_t_to_local_i = self.info_nce(chunk_embeddings, patch_embeddings)\n",
    "\n",
    "        loss_local_i_to_global_t = self.info_nce(patch_embeddings, text_embeddings)\n",
    "        loss_local_t_to_global_i = self.info_nce(chunk_embeddings, image_embeddings)\n",
    "        loss_global_i_to_local_t = self.info_nce(image_embeddings, chunk_embeddings)\n",
    "        loss_global_t_to_local_i = self.info_nce(text_embeddings, patch_embeddings)\n",
    "\n",
    "        loss_global_t_to_global_i = self.info_nce(text_embeddings, image_embeddings)\n",
    "        loss_global_i_to_global_t = self.info_nce(image_embeddings, text_embeddings)\n",
    "        \n",
    "        # Combined loss\n",
    "        combined_loss = (loss_local_i_to_local_t + loss_local_t_to_local_i \\\n",
    "                        + loss_local_i_to_global_t + loss_local_t_to_global_i \\\n",
    "                        + loss_global_i_to_local_t + loss_global_t_to_local_i \\\n",
    "                        + loss_global_t_to_global_i + loss_global_i_to_global_t) / 8\n",
    "        \n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_batch = torch.zeros(2, 1, 128)\n",
    "test_patch_batch = torch.zeros(2, 2, 9, 128)\n",
    "test_text_batch = torch.ones(2, 1, 128)\n",
    "test_chunks_batch = torch.ones(2, 2, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = LocalContrastiveLoss(img_model, text_model).to(device)\n",
    "loss = gen_model(test_image_batch, test_patch_batch, test_text_batch, test_chunks_batch)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
